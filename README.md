<div align="center">
  <h1>ðŸ“š BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation </h1>
</div>


**Repository for Anonymous Review of [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482).**


## ðŸ“Œ Overview

**BenchHub** is a unified benchmark suite designed to help researchers and developers **easily load, filter, and process various LLM benchmark datasets**.

It enables efficient dataset handling for **training and evaluation**, providing flexible filtering capabilities by:
- **Subject**
- **Skill**
- **Target**

This allows users to build **custom benchmarks** tailored to specific needs and conduct **holistic evaluations** of language models.

## ðŸ”§ Features

The evaluation system in `BenchHub` provides key utilities for conducting standardized LLM evaluation. 

We currently support three main function for evaluation:

1. **Flexible Benchmark Loader (`load_benchhub`,`classify_intent` )**  
  - The primary interface for filtering and loading evaluation datasets.  
   Users can directly specify `skill`, `target`, and `subject` filters to construct customized evaluation subsets.
  - ðŸ§  Additionally, BenchHub offers an **intent classification** module that maps free-form evaluation goals (e.g., *"Evaluate understanding of Korean culture"*) to structured filters based on BenchHub's taxonomy.
  <!-- ### Example: `load_dataset` Function

  You can load and filter datasets using the `load_benchhub` function. Here's how to use it:

  ```python
  from src import load_benchhub

  df = load_benchhub(
      lang='kor',                # Specify language (e.g., 'kor' for Korean)
      subject=['history', 'math'],  # Filter based on subjects
      skill='reasoning',         # Filter based on skill type
      target='general',          # Filter based on target type
      save='filtered_dataset.csv' # Optionally save the filtered dataset to a CSV file
  )
  ``` -->


2. **Evaluation Execution (via HRET)**  
  - BenchHub works in conjunction with the [HRET (Haerae Evaluation Toolkit)](https://github.com/HAE-RAE/haerae-evaluation-toolkit/) evaluation toolkit to support evaluations using string-matching, LLM inference, or external scoring. This collaboration ensures compatibility with broader evaluation pipelines.

3. **Citation Report Generator**  
  - As BenchHub serves mixture of benchmarks, we provide automated LaTeX + BibTeX reports that document the dataset sources included in your evaluation. This supports transparent reporting in research papers.

4. **Category-Wise Performance Analysis (TBA)**
  - A future module will support category-wise accuracy analysis, allowing users to break down evaluation results by skill type, subject area, or benchmark source.

> ðŸ“‚ For full usage and code examples, see [`evaluation/README.md`](evaluation/README.md)



## Implementation of BenchHub 

BenchHub is designed to transform raw benchmarks into BenchHub-compatible formats and to automatically categorize datasets by skill, subject, and target using both rule-based and LLM-guided methods. The code for reformatting and categorization is located in the /pipeline directory, with more details available in [`pipeline/README.md`](pipeline/README.md)
<!-- * **`agents/run.py`**: An end-to-end reformatter based on an agent-driven architecture. It automates the process of reformatting datasets for model training and evaluation in a flexible, scalable manner.
* **`agents/run_determ_github.py`**: A rule-based, LLM-guided reformatter designed specifically for datasets from GitHub. It leverages rule-based logic to process and format the data for easier analysis.
* **`agents/run_determ.py`**: A rule-based, LLM-guided reformatter focused on datasets from Hugging Face. It applies rule-based techniques to preprocess and format Hugging Face datasets for downstream tasks. -->


